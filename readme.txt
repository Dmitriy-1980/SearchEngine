
ConfigAppl :
Класс содержит переменные задаваемые черех кофигурайционный файл application.yml.
Поля:
deeplimit - глубина сканирования ссылок. Главная страница=1, ссылки с нее имеют глубину=2,
    ссылки с них имеют глубину=3 и тд. Установленное значения =0 означает отсутствие ограничений.
    Сама переменная нужна при отладке, что бы ограничить количество обрабатываемых страниц.
sites - список сайтов полученный изконфигурационного файла.
userAgent - подставное имя поискового бота
referer - подставной источник запроса
timeout -таймаут(ms) между обращениями к сайту
responseWait - время одижания ответа индексируемой страницы
maxFrequency - макс кол страниц с искомой леммой. (отсев чрезмерно распространенных)


public class Indexing
Осуществляет попытку запуска индексации - запуск задачи PageParser.
В зависимости от задачи запуск с разнымипараметрами.
(запуск всего сайта или отдельной страницы)
Определение параметров и запуск задачи в пуле потоков происходит в методе Indexing.goSiteIndex();

При парсинге собирается список ссылок найденных на странице.
Для каждой найденной "правильной" ссылки создает новую задачу.
В список добавляется толькоте ссылки , которых еще не было в рамках этого сайта.
Для этого до конца парсинга сайта существует коллекция linkSet.
	Она сквозная для сайта- создается с первой страницей и передается параметром для каждой дочерней.
Аналогичная задача у коллекции siteLemmaMap<лемма, кол страниц с ней>
(суть задумки - уменьшить количествообращений к БД.)

Так же при старте парсинга запускается отдельная задача WaitOfIndexEnd
    которая следит за списком задачь.
    Список задач регулярно проверяется и из него удаляются завершенные.
    Как только он опустеет - он удаляется.
    Когда будут удалены все списки- закончена индексация.



public class PageParser extends RecursiveAction
    private final String pageUrl; - адрес текущей страницы
    private Boolean onlyThisPage = false; - флаг показывающи, что индексация только этой страницы,
        без проваливания вниз по ссылкам.
    private SiteEntity site; - сущность сайта - формируется при чтении главной страницы.
        При "проваливании" вниз по ссылкам передается "дочерним" парсерам параметром.
    private Document document; - структурный документ, Jsoup
    private final Vector<String> linksSet; - лист ссылок всего сайта.
        При "проваливании" вниз по ссылкам передается "дочерним" парсерам параметром.
    private final ConcurrentHashMap<String, Integer> siteLemmaMap; -
        Коллекция лемм всего сайта с кол. страниц их содержащих.
        При "проваливании" вниз по ссылкам передается "дочерним" парсерам параметром.
    private final ConcurrentHashMap<String, List<RecursiveAction>> taskList; -
        Коллекция с задачами для сайта. Ключ-url сайта, Значения - список задач.
    private final ForkJoinPool pool;
    private final Integer deep; - "глубина" текущей страницы относительно главной.
        (глубина главной = 1)
    private String protocol; - протокол http/https
    private final ConfigAppl config; - бинс конфигурацией приложения.
    private final SiteService siteService; - серия сервисов
    private final PageService pageService;
    private final LemmaService lemmaService;
    private final IndexService indexService;
    private final LuceneService luceneService;
    private final MyLog log = new MyLog() - логгер.

    /*В конструктор передаются сервисы, конфиг приложения, пулл потоков - всегда одинаково.
    * deep-глубина вложенности обрабатываемой ссылки. Если deep==1 то это главная страница.
    * linkSet и siteLemmaMap коллекции ссылок и лемм одного сайта.Они сквозные для всего сайта
    * и нужны для контроля на уникальность ссылок и подсчета лемм без обращения к SQL-серверу.
    * taskList- структура для отслеживания состояния задач. Вполнена, остановлена, ошибка*/




public class Search
выполнить поисковый запрос:
1 получить список ID подходящих страниц
2 пересчитать ранги и сформировать список объектов с даными по страницам
3 получить сниппеты
4 формировать ответ заданного формата

